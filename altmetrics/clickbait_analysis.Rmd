---
title: "Almetrics Polaroty Analysis"
author: "Karl Holub <karl.holub@nested-knowledge.com>"
date: "4/10/2019"
output: html_document
---

```{r setup, include=FALSE}
Sys.setenv(crossref_email='karljholub@gmail.com')
studies <- read.csv('./static/studies.csv')
sampled_dois <- readLines(file('./static/sampled_dois.txt'))
```

## Thank you Altmetric (TM) (R) (C)
![Almetric Logo](static/altmetric_tm_r_c.png)
[As requested by Altmetric.com](http://api.altmetric.com/index.html#attribution) for any consumer of their data, I am prominently featuring Altmetric.com branding on this document. They are "a research metrics company who track and collect the online conversations around millions of scholarly outputs". You can learn more about their attention score [here](https://www.altmetric.com/about-our-data/the-donut-and-score/) and some of its computational details [here](https://help.altmetric.com/support/solutions/articles/6000060969-how-is-the-altmetric-score-calculated-) (note that a tweet is worth four Facebook posts -- take that Zuckerberg!).

## About
Almetrics are a measurement of proliferation of scholarly works in online environments. These environments include social media, professional and educational platforms, and policy documents. These relatively informal environments stand in contrast to peer-reviewed publication and citation, used to compute impact factor (IF) and traditionally held as a proxy to research quality.

Altmetric [clearly states](https://help.altmetric.com/support/solutions/articles/6000137149-guide-for-describing-altmetric-data) that their score measures attention and specifically does not attempt to proxy impact. [Cassidy Sugimoto hypothesizes](https://www.wiley.com/network/researchers/promoting-your-article/attention-is-not-impact-and-other-challenges-for-altmetrics) that "the nature of this attention is something much more complex and far less understood" compared to impact as measured by citation counts. In other words, the linkage between citation count in peer-reviewed journals and impact is much more direct than mentions in open sources and impact. This can, for example, be attributed to non-experts weighing in online (which of isn't necessarily a bad thing, just suggests higher false positive rate) or a wider audience on online channels for possible "fluke" virality.

I find the ambiguation plausible and useful, but I am also doubtful that consumers of bibliometrics will utilize them for different purpose. It is too tempting to simplify and interpret them as similar measures of research quality. Indeed, [prior research](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0064841) found Altmetric Attention Score and citation count to be correlated.

### Objective
If the Altmetric Attention Score is to be regarded similarly to Impact Factor, we should understand its operation and failure modes. 

In print publication "click bait" titles are summarizations or headlines designed to catch attention and stir controversy. With this method, publishers are hoping to drive traffic to their content (e.g. for ad revenues). Could a similar phenomenon exist in academic publication? *That is, could research with a flashy, polarizing, or controversial title drive increased online attention relative to similar content research with a more mild title?* The causal mechanisms include:

* the internet exposes a high volume of information; most people don't have time to ingest it all, so a title may be all a browser uses to make a determination about the study.
* the internet, and particularly social media, is largely used for entertainment purposes. Users may seek only to share or mention content that is flashy or controversial, resulting in title bias.
* non-experts are the audience on the wider internet; they may not have a good sense of what is realistic or standard in a field.

Implicit to this hypothesis is that IF doesn't suffer a similar title bias; unfortunately, at time of writing, I don't have IF data to perform this analysis.

## The Data
Two sources were used to pull data - one for random sampling of studies and another for obtaining Attention Scores for the sample.

### Crossref API
Crossref is a DOI authority and houses pointers to 90 million + digital objects. There may be some bias by virtue of what journals submit for DOI assignment using Crossref. We will ignore this potential bias. The [Crossref API](https://github.com/CrossRef/rest-api-doc) and [rcrossref](https://github.com/ropensci/rcrossref) can be used to fetch randomly hosted DOIs in the Crossref database.

```{r eval=FALSE}
# we only consider published after 2014 before 2018, so that the article exists in Altmetric's timeline and allow time for online mentions to build
sampled_dois_deep <- lapply(1:5000, function(x) { 
                                      if (x %% 100 == 0) {
                                        print(paste0('On iteration: ', x))
                                      }
                                      Sys.sleep(.1)
                                      cr_r(sample=100,
                                       filters=c(until_created_date = '2018-01-01', from_created_date='2014-01-01'))})

sampled_dois <- sampled_dois_deep %>%
  unlist() %>%
  unique() # some of the sample may be duplicate, given that there are only 90M DOIs on Crossref
writeLines(sampled_dois, file('./static/sampled_dois.txt'))
```

### Almetric API
Almetric exposes their attention score in a publically available API, keyed by DOI. Using the random sample of DOIs obtained from Crossref, we will gather attention scores.
```{r eval=FALSE}
# sleep a scecond to play nicely with Altmetric's API 
# loop instead of vectorized so that intermediate results aren't lost.
json_dumps <- c()
total_requests <- 1
for (doi in sampled_dois[63256:length(sampled_dois)]) {
  Sys.sleep(1)
  if (total_requests %% 10000 == 0) {
      writeLines(json_dumps, file('./static/raw_json_save.txt'))
  }
  json_dumps <- c(json_dumps, getURL(paste0('https://api.altmetric.com/v1/doi/', doi)))
  total_requests <- total_requests + 1
}

# since not all DOIs are academic works, and some academic works are not listed on Altmetric
json_dumps_with_score <- json_dumps[json_dumps != 'Not Found']
parsed_json <- sapply(json_dumps_with_score, fromJSON) %>%
  unname() %>%
  lapply(function(s) { list(
    doi = s$doi,
    title = s$title[[1]],
    journal = s$journal,
    last_updated_epoch = s$last_updated,
    publisher_field = s$publisher_subjects$name[1],
    paper_field = s$scopus_subjects[1],
    attention_score = s$score)})

studies <- do.call(rbind, parsed_json) %>% as.data.frame()
# R is goofy in NULL handling in dataframes, so we swap out NAs and coerce to columns to vectors
for (col in colnames(studies)) { 
  studies[[col]] <- sapply(studies[[col]], function(x) {
    if (is.null(x)) {
      return(NA)
    }
    return(x)
  })
}
write.table(studies, './static/studies.csv', row.names = FALSE, col.names = TRUE, sep=',')
```
Note that the the Crossref API cannot be seeded and Altmetric is mutable and takes days to extract data from, so the sample used to generate this document was fixed and checked into version control.

## Methods
Text parsing and sentiment analysis is primarily done using the [sentimentr library](https://github.com/trinker/sentimentr) which uses 

### Text Refinement
Titles were parsed with the following approach:

* Break the title into words using whitespace
* Throw away non-pause punctuation

### Sentiment Analysis
We are interested in assessing the *polarity* of text. Polarity is a measurement of the positive or negative emotion text conveys. It is hypothesized that the magnitude of polarity (in either the positive or negative direction) is a good proxy for how controversial, flashy, or dramatized a title is.

There is a diversity of methods for assessing polarity. None of these are certifiably "right" due to the unsupervised nature of the problem. Two approaches are considered.

#### Absolute Polarity
Most methods of polarity analysis are based on a pre-built dictionary of words and an associated polarity score. For example, [Matthew Jocker's](https://github.com/mjockers/syuzhet) `syuzhet` dictionary looks like:

```{r}
set.seed(55455)
sample_n(syuzhet::get_sentiment_dictionary(), 10)
```
"infiltrated" and "enslaves" carry highly *negative* connotations in common usage and have low scores; "renovate" and "enhanced" carry *positive* connotations and have positive scores. 

Using such a dictionary, a simple method to assess polarity of a title is to:

* Lookup the score for each word in a title
    * throw away words not in the polarity dictionary
* Sum up the scores for each title
    * in our case, the absolute value of scores are summed
* Normalize by the number of words in the title
  
The choice to sum aboslute scores is to capture general strength of language. For example, the phrase "Pandas are happily teetering on the edge of a peaceful extinction" is fairly balanced (happily/peaceful cancelling out teetering/extinction) while overall using fairly colorful, polarizing language.

#### Whole Title Polarity
The "augmented dictionary method" of `sentimentr` is a "context aware" version of the above. It considers the context (surrounding 2-5 words) as "valence shifters" in its assessment of polarity. Valence shifters can be:

* Word amplifiers: "extremely" or "very" which should increase the score of a word
    * Additionally, de-amplifiers such as "hardly" and "minimally" decrease the score of a word.
* Negators: "not", "less" which invert the polarity of a word 
* And more

This strategy is used to produce a per-title score

## Results

```{r}
dictionary <- syuzhet::get_sentiment_dictionary()

score_joined_individual_words <- studies %>%
  select(doi, title) %>%
  tidytext::unnest_tokens(word, title) %>%
  anti_join(tidytext::get_stopwords('en'), by = 'word') %>%
  inner_join(dictionary, by = 'word')

doi_to_sap <- score_joined_individual_words %>%
  group_by(doi) %>%
  summarize(
    sap = mean(abs(value))
  )

titles <- as.list(studies$title)
class(titles) <- c('list', 'get_sentences', 'get_sentences_character')
doi_to_wtp <- studies %>% select(doi)
doi_to_wtp$wtp <- abs(sentimentr::sentiment(titles)$sentiment)

score_joined_studies <- studies %>%
  left_join(doi_to_sap, by = 'doi') %>%
  left_join(doi_to_wtp, by = 'doi') %>%
  select(doi, title, attention_score, paper_field, sap, wtp)

score_joined_studies %>% arrange(-sap) %>% head()

with(score_joined_studies %>% filter(!is.na(sap)), cor(sap, attention_score, method = 'spearman'))
with(score_joined_studies %>% filter(!is.na(wtp)), cor(wtp, attention_score, method = 'spearman'))
with(score_joined_studies %>% filter(!is.na(wtp) & !is.na(sap)), cor(wtp, sap, method = 'spearman'))

# since it's likely that different fields have different magnitudes of online attention
score_joined_studies %>% 
  filter(!is.na(wtp)) %>%
  group_by(paper_field) %>%
  filter(n() > 50) %>%
  summarize(
    correlation = cor(wtp, attention_score, method = 'spearman'),
    papers = n()
  )

score_joined_studies %>% 
  filter(!is.na(sap)) %>%
  group_by(paper_field) %>%
  filter(n() > 50) %>%
  summarize(
    correlation = cor(sap, attention_score, method = 'spearman'),
    papers = n()
  )
```

### Visualizing and Understanding the Sample

### Correlating Polarity and Attention Score 


## Conclusion
Quoting Sugimoto again, "Altmetrics should be harnessed not to replace any existing metrics, but rather to expand the tools available to demonstrate the diffusion of science."